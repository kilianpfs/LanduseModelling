{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08b1376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from typing import Callable, List, Union, Tuple, Dict\n",
    "from pathlib import Path\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e62d6",
   "metadata": {},
   "source": [
    "## Reading in Catchment Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ebe2c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_catchment_ts(runoff_path: str, catchment_path: str, catchment_name: str) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Optimized loading and preprocessing of time series data.\n",
    "    \n",
    "    Args:\n",
    "        runoff_path: Path to GRDC runoff data\n",
    "        catchment_path: Directory containing catchment climate data\n",
    "        catchment_name: Name of the catchment\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed xarray Dataset with aligned time series\n",
    "    \"\"\"\n",
    "    # Convert paths to Path objects for better handling\n",
    "    catchment_path = catchment_path + \"/\" + catchment_name.upper()\n",
    "    catchment_path = Path(catchment_path)\n",
    "    \n",
    "    # 1. Load runoff data with optimized parameters\n",
    "    runoff = xr.open_dataset(\n",
    "        runoff_path,\n",
    "        chunks={'time': 'auto'},  # Enable chunking for better memory management\n",
    "        cache=True              # Cache the data for faster repeated access\n",
    "    ).load()\n",
    "    \n",
    "    # 2. Immediately select time period and station\n",
    "    runoff = runoff.sel(\n",
    "        time=slice('2000-03-01', '2022-12-19')\n",
    "    )\n",
    "\n",
    "    catchment_id = runoff[\"station_name\"].values == catchment_name.upper()\n",
    "    catchment_index = np.where(catchment_id)[0][0]\n",
    "\n",
    "    # Select only the runoff timeseries\n",
    "    runoff = runoff[\"runoff_mean\"].isel(id=catchment_index)\n",
    "    \n",
    "    # 3. Use dictionary comprehension for cleaner climate data loading\n",
    "    climate_vars = {\n",
    "        'temperature': 't2m',\n",
    "        'precipitation': 'precipitation',\n",
    "        'radiation': 'nr',\n",
    "        'ndvi': 'ndvi'\n",
    "    }\n",
    "    \n",
    "    # 4. Parallel loading of climate data\n",
    "    climate_ds = xr.open_mfdataset(\n",
    "        [catchment_path/f'{var}.nc' for var in climate_vars],\n",
    "        parallel=True,  # Enable parallel loading\n",
    "        combine='by_coords',\n",
    "        chunks={'time': 'auto'},\n",
    "        cache=True\n",
    "    ).load()\n",
    "    \n",
    "    # 5. Create output dataset with aligned data\n",
    "    data = xr.Dataset({\n",
    "        'temperature': climate_ds[climate_vars['temperature']],\n",
    "        'precipitation': climate_ds[climate_vars['precipitation']],\n",
    "        'radiation': climate_ds[climate_vars['radiation']],\n",
    "        'ndvi': climate_ds[climate_vars['ndvi']],\n",
    "        'observed': runoff.broadcast_like(climate_ds[climate_vars['radiation']])\n",
    "    })\n",
    "    \n",
    "    # 6. Ensure consistent time alignment\n",
    "    data = data.sel(time=slice('2001-01-01', '2021-12-31'))\n",
    "    \n",
    "    # 7. Convert to float32 to save memory (if precision is sufficient)\n",
    "    #for var in data.data_vars:\n",
    "    #    data[var] = data[var].astype(np.float32)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4398615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_and_preprocess_catchment_ts(\n",
    "    runoff_path=\"data/catchments/GRDC-Daily.nc\",\n",
    "    catchment_path=\"data/catchment_timeseries\",\n",
    "    catchment_name=\"Bentfeld\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d67e9",
   "metadata": {},
   "source": [
    "## Simple Water Balance Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5247d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(fastmath=True)\n",
    "def time_evolution_numba(temp, rad, prec, ndvi, c_s, alpha, beta, gamma, c_m, iota, temp_w, ndvi_w):\n",
    "    #Initialize \n",
    "    length = len(temp)\n",
    "    runoff_out = np.full(length, np.nan)\n",
    "    evapo_out = np.full(length, np.nan)\n",
    "    soil_mois_out = np.full(length, np.nan)\n",
    "    snow_out = np.full(length, np.nan)\n",
    "\n",
    "    # Transformations / Calculations for Setup\n",
    "    conv = 1 / 2260000  # from J/day/m**2 to mm/day\n",
    "    rad = rad * conv  # convert radiation to mm/day\n",
    "    prec = prec * 10 **3 # from m/day to mm/day\n",
    "    w = 0.9 * c_s\n",
    "    snow = 0\n",
    "\n",
    "    # --- calc_et_weight function ---\n",
    "    ndvi = np.nan_to_num(ndvi, nan=0.0)\n",
    "    normalized_temp = (temp - temp.min()) / (temp.max() - temp.min())\n",
    "    normalized_ndvi = (ndvi - ndvi.min()) / (ndvi.max() - ndvi.min())\n",
    "    et_weight = temp_w * normalized_temp + ndvi_w * normalized_ndvi\n",
    "    beta_weighted = beta * et_weight\n",
    "\n",
    "    for t in range(1, length):\n",
    "        prec_t = prec[t-1]\n",
    "        temp_t = temp[t-1]\n",
    "        rad_t = rad[t-1]\n",
    "        beta_t = beta_weighted[t-1]\n",
    "\n",
    "        # ---- snow_function ----\n",
    "        is_melting = temp_t > 273.15\n",
    "        has_snow = snow >= 0.001\n",
    "\n",
    "        if not is_melting:\n",
    "            snow += prec_t\n",
    "            water = 0.0\n",
    "        elif is_melting and has_snow:\n",
    "            melt = c_m * (temp_t - 273.15)\n",
    "            melt = min(melt, snow)\n",
    "            snow -= melt\n",
    "            water = melt + prec_t\n",
    "        else:\n",
    "            water = prec_t\n",
    "\n",
    "        runoff = (water + iota) * (w / c_s) ** alpha\n",
    "        evap = beta_t * (w / c_s) ** gamma * rad_t\n",
    "\n",
    "        w += (water - runoff - evap)\n",
    "        w = np.maximum(w, 0.0)\n",
    "\n",
    "        # Store results\n",
    "        runoff_out[t] = runoff\n",
    "        evapo_out[t] = evap\n",
    "        soil_mois_out[t] = w\n",
    "        snow_out[t] = snow\n",
    "\n",
    "    return runoff_out, evapo_out, soil_mois_out, snow_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0987fa83",
   "metadata": {},
   "source": [
    "## Finding best params using GA\n",
    "\n",
    "### Evaluator using Pearsons R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c26bb2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def compute_correlation_numba(observed: np.ndarray, simulated: np.ndarray) -> np.float32:\n",
    "    \"\"\"Numba-optimized Pearson correlation calculation.\"\"\"\n",
    "    # Create mask for valid observations\n",
    "    mask = (~np.isnan(observed)) & (~np.isnan(simulated))\n",
    "    valid_count = np.sum(mask)\n",
    "    \n",
    "    if valid_count < 2:\n",
    "        return -np.inf  # Return negative infinity for invalid cases\n",
    "    \n",
    "    # Extract valid values\n",
    "    x = observed[mask]\n",
    "    y = simulated[mask]\n",
    "    \n",
    "    # Compute means\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    \n",
    "    # Center the data\n",
    "    x_centered = x - x_mean\n",
    "    y_centered = y - y_mean\n",
    "    \n",
    "    # Compute correlation components\n",
    "    numerator = np.dot(x_centered, y_centered)\n",
    "    denominator = np.sqrt(np.dot(x_centered, x_centered) * np.dot(y_centered, y_centered))\n",
    "    \n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def create_evaluator(temp, rad, prec, ndvi, observed):\n",
    "    \"\"\"Factory function with consistent types.\"\"\"\n",
    "    # Convert inputs to float32 and ensure contiguous\n",
    "    temp = np.ascontiguousarray(temp, dtype=np.float32)\n",
    "    rad = np.ascontiguousarray(rad, dtype=np.float32)\n",
    "    prec = np.ascontiguousarray(prec, dtype=np.float32)\n",
    "    ndvi = np.ascontiguousarray(ndvi, dtype=np.float32)\n",
    "    observed = np.ascontiguousarray(observed, dtype=np.float32)\n",
    "\n",
    "    def evaluate_individual(params):\n",
    "        \"\"\"Type-consistent evaluation.\"\"\"\n",
    "        c_s, alpha, beta, gamma, c_m, iota, temp_w, ndvi_w = params\n",
    "        \n",
    "        simulated = time_evolution_numba(\n",
    "            temp=temp,\n",
    "            rad=rad,\n",
    "            prec=prec,\n",
    "            ndvi=ndvi,\n",
    "            c_s=c_s,\n",
    "            alpha=alpha,\n",
    "            beta=beta,\n",
    "            gamma=gamma,\n",
    "            c_m=c_m,\n",
    "            iota=iota,\n",
    "            temp_w=temp_w,\n",
    "            ndvi_w=ndvi_w\n",
    "        )[0]\n",
    "\n",
    "        return compute_correlation_numba(observed, simulated)    \n",
    "    return evaluate_individual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18497b08",
   "metadata": {},
   "source": [
    "## Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_population(\n",
    "    pop_size: int,\n",
    "    genome_length: int,\n",
    "    lower_bounds: List[float],\n",
    "    upper_bounds: List[float],\n",
    "    quantization_steps: int = 20\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Initialize a population uniformly within given bounds.\n",
    "    \n",
    "    Args:\n",
    "        pop_size: Number of individuals in population\n",
    "        genome_length: Length of each genome\n",
    "        lower_bounds: List of lower bounds for each gene\n",
    "        upper_bounds: List of upper bounds for each gene\n",
    "        quantization_steps: Number of discrete steps for genome space\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (population array, genome_space array)\n",
    "    \"\"\"\n",
    "    # Convert bounds to numpy arrays (faster than checking types)\n",
    "    lower = np.asarray(lower_bounds, dtype=np.float64)\n",
    "    upper = np.asarray(upper_bounds, dtype=np.float64)\n",
    "    \n",
    "    # Generate population in one vectorized operation\n",
    "    population = np.random.uniform(\n",
    "        low=lower,\n",
    "        high=upper,\n",
    "        size=(pop_size, genome_length)\n",
    "    )\n",
    "    \n",
    "    # Create genome space for quantization\n",
    "    # Transpose to get shape (genome_length, quantization_steps)\n",
    "    genome_space = np.linspace(lower, upper, quantization_steps).T\n",
    "    \n",
    "    return population, genome_space\n",
    "\n",
    "def evaluate_fitness(\n",
    "    population: np.ndarray,\n",
    "    evaluator: Callable[[np.ndarray], float]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Evaluate fitness for each individual in the population.\n",
    "    Vectorized implementation if fitness_func supports it.\n",
    "    \"\"\"\n",
    "    fitnesses = np.empty(len(population), dtype=np.float32)\n",
    "    for i in prange(len(population)):\n",
    "        fitnesses[i] = evaluator(population[i])\n",
    "    return fitnesses\n",
    "\n",
    "def select_survivors(\n",
    "    population: np.ndarray,\n",
    "    fitnesses: np.ndarray,\n",
    "    num_parents: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Roulette wheel (fitness-proportionate) selection.\n",
    "    Vectorized implementation.\n",
    "    \"\"\"\n",
    "    probs = fitnesses / np.sum(fitnesses)\n",
    "    indices = np.random.choice(len(population), size=num_parents, replace=True, p=probs)\n",
    "    return population[indices]\n",
    "\n",
    "def pop_crossover(\n",
    "    pop: np.ndarray,\n",
    "    crossover_rate: float = 0.8\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized single-point crossover for entire population.\n",
    "    \"\"\"\n",
    "    pop_size, genome_length = pop.shape\n",
    "    if genome_length < 2:\n",
    "        return pop.copy()\n",
    "    \n",
    "    # Create mask for which individuals will crossover\n",
    "    crossover_mask = np.random.rand(pop_size // 2) < crossover_rate\n",
    "    num_crossovers = np.sum(crossover_mask)\n",
    "    \n",
    "    if num_crossovers == 0:\n",
    "        return pop\n",
    "    \n",
    "    # Select parents and prepare children\n",
    "    parents1 = pop[:num_crossovers]\n",
    "    parents2 = pop[num_crossovers:2*num_crossovers]\n",
    "    \n",
    "    # Vectorized crossover\n",
    "    crossover_points = np.random.randint(1, genome_length, size=num_crossovers)\n",
    "    mask = np.arange(genome_length) < crossover_points[:, None]\n",
    "    \n",
    "    children1 = np.where(mask, parents1, parents2)\n",
    "    children2 = np.where(mask, parents2, parents1)\n",
    "    \n",
    "    # Put children back in population\n",
    "    pop[:num_crossovers] = children1\n",
    "    pop[num_crossovers:2*num_crossovers] = children2\n",
    "    \n",
    "    return pop\n",
    "\n",
    "def pop_mutate(\n",
    "    pop: np.ndarray,\n",
    "    genome_space: np.ndarray,\n",
    "    mutation_rate: float,\n",
    "    mutation_scale: float = 0.1\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized mutation with optional quantization.\n",
    "    \"\"\"\n",
    "    pop_size, genome_length = pop.shape\n",
    "    num_mutations = int(mutation_rate * pop_size * genome_length)\n",
    "    \n",
    "    if num_mutations == 0:\n",
    "        return pop\n",
    "    \n",
    "    # Create random mutation indices\n",
    "    mut_indices = np.random.choice(pop_size * genome_length, size=num_mutations, replace=False)\n",
    "    rows, cols = np.unravel_index(mut_indices, pop.shape)\n",
    "    \n",
    "    # Apply mutations\n",
    "    if genome_space is not None:\n",
    "        # Quantized mutation\n",
    "        which_steps = np.random.randint(0, genome_space.shape[1], size=num_mutations)\n",
    "        pop[rows, cols] = genome_space[cols, which_steps]\n",
    "    else:\n",
    "        # Gaussian mutation\n",
    "        lower = genome_space[:, 0] if genome_space is not None else np.min(pop, axis=0)\n",
    "        upper = genome_space[:, -1] if genome_space is not None else np.max(pop, axis=0)\n",
    "        ranges = upper - lower\n",
    "        noise = np.random.normal(scale=mutation_scale * ranges[cols], size=num_mutations)\n",
    "        pop[rows, cols] = np.clip(pop[rows, cols] + noise, lower[cols], upper[cols])\n",
    "    \n",
    "    return pop\n",
    "\n",
    "def select_survivors_elitism(\n",
    "    combined_pop: np.ndarray,\n",
    "    combined_fit: np.ndarray,\n",
    "    pop_size: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    (mu + lambda) survivor selection: keep best individuals.\n",
    "    Vectorized implementation.\n",
    "    \"\"\"\n",
    "    idx = np.argsort(combined_fit)[-pop_size:]\n",
    "    return combined_pop[idx], combined_fit[idx]\n",
    "\n",
    "def run_ga(\n",
    "    fitness_func: Callable[[np.ndarray], float],\n",
    "    genome_length: int,\n",
    "    lower_bounds: Union[float, List[float]],\n",
    "    upper_bounds: Union[float, List[float]],\n",
    "    pop_size: int = 50,\n",
    "    num_generations: int = 100,\n",
    "    num_parents: int = 30,\n",
    "    crossover_rate: float = 0.8,\n",
    "    mutation_rate: float = 0.1,\n",
    "    mutation_scale: float = 0.1,\n",
    "    use_quantization: bool = False,\n",
    "    seed: int = None\n",
    ") -> Tuple[np.ndarray, List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Optimized genetic algorithm with vectorized operations.\n",
    "    Returns (best_individual, best_fitness_history, avg_fitness_history).\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Initialize population\n",
    "    quantization_steps = 20 if use_quantization else 0\n",
    "    population, genome_space = initialize_population(\n",
    "        pop_size, genome_length, lower_bounds, upper_bounds, quantization_steps\n",
    "    )\n",
    "    fitnesses = evaluate_fitness(population, fitness_func)\n",
    "    best_hist = []\n",
    "    avg_hist = []\n",
    "\n",
    "    for gen in range(num_generations):\n",
    "        # Record stats\n",
    "        best_hist.append(np.max(fitnesses))\n",
    "        avg_hist.append(np.mean(fitnesses))\n",
    "\n",
    "        # Selection\n",
    "        parents = select_survivors(population, fitnesses, num_parents)\n",
    "        \n",
    "        # Create offspring by copying parents\n",
    "        offspring = parents.copy()\n",
    "        \n",
    "        # Crossover\n",
    "        offspring = pop_crossover(offspring, crossover_rate)\n",
    "        \n",
    "        # Mutation\n",
    "        offspring = pop_mutate(\n",
    "            offspring, \n",
    "            genome_space if use_quantization else None,\n",
    "            mutation_rate,\n",
    "            mutation_scale\n",
    "        )\n",
    "        \n",
    "        # Evaluate offspring\n",
    "        offspring_fit = evaluate_fitness(offspring, fitness_func)\n",
    "        \n",
    "        # Survivor selection (elitism)\n",
    "        combined_pop = np.vstack((population, offspring))\n",
    "        combined_fit = np.concatenate((fitnesses, offspring_fit))\n",
    "        population, fitnesses = select_survivors_elitism(combined_pop, combined_fit, pop_size)\n",
    "\n",
    "    # Final stats\n",
    "    best_hist.append(np.max(fitnesses))\n",
    "    avg_hist.append(np.mean(fitnesses))\n",
    "    best_idx = np.argmax(fitnesses)\n",
    "    best_individual = population[best_idx]\n",
    "\n",
    "    return best_individual, best_hist, avg_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b91d3ff",
   "metadata": {},
   "source": [
    "## Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "638d5985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bootstrap_sample(dataset: xr.Dataset, sampled_years: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Process a single bootstrap sample (to be parallelized).\"\"\"\n",
    "    sample_data = {}\n",
    "    for var in dataset.data_vars:\n",
    "        chunks = []\n",
    "        for year in sampled_years:\n",
    "            year_data = dataset[var].sel(time=dataset.time.dt.year == year)\n",
    "            chunks.append(year_data)\n",
    "        sample_data[var] = xr.concat(chunks, dim='time').values\n",
    "    return sample_data\n",
    "\n",
    "def parallel_year_bootstrap(dataset: xr.Dataset, \n",
    "                          n_samples: int = 10, \n",
    "                          n_jobs: int = -1) -> List[Dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Parallel year-based block bootstrap.\n",
    "    \n",
    "    Args:\n",
    "        dataset: xarray Dataset containing all variables\n",
    "        n_samples: Number of bootstrap samples\n",
    "        n_jobs: Number of parallel jobs (-1 for all cores)\n",
    "        \n",
    "    Returns:\n",
    "        List of bootstrap samples\n",
    "    \"\"\"\n",
    "    years = pd.to_datetime(dataset.time.values).year\n",
    "    unique_years = np.unique(years)\n",
    "    n_years = len(unique_years)\n",
    "    \n",
    "    # Generate all year samples upfront\n",
    "    year_samples = [np.random.choice(unique_years, size=n_years, replace=True) \n",
    "                   for _ in range(n_samples)]\n",
    "    \n",
    "    # Process in parallel with progress bar\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_bootstrap_sample)(dataset, years) \n",
    "        for years in tqdm(year_samples, desc=\"Bootstrapping\"))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f273de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_ga_wrapper(sample: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Wrapper function for parallel GA execution.\"\"\"\n",
    "    evaluator = create_evaluator(\n",
    "        temp=sample['temperature'],\n",
    "        rad=sample['radiation'],\n",
    "        prec=sample['precipitation'],\n",
    "        ndvi=sample['ndvi'],\n",
    "        observed=sample['observed']\n",
    "    )\n",
    "    \n",
    "    return run_ga(\n",
    "                fitness_func=evaluator,\n",
    "                genome_length=8,\n",
    "                lower_bounds=lowerBounds,\n",
    "                upper_bounds=upperBounds,\n",
    "                pop_size=100,\n",
    "                num_generations=20,\n",
    "                num_parents=40,\n",
    "                crossover_rate=0.9,\n",
    "                mutation_rate=0.05,\n",
    "                mutation_scale=0.2,\n",
    "                seed=42\n",
    "            )\n",
    "\n",
    "def run_parallel_ga_bootstrap(\n",
    "    dataset: xr.Dataset,\n",
    "    n_bootstraps: int = 10,\n",
    "    n_jobs: int = -1\n",
    ") -> Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Fully parallelized GA with bootstrap.\n",
    "    \n",
    "    Args:\n",
    "        dataset: xarray Dataset\n",
    "        n_bootstraps: Number of bootstrap samples\n",
    "        ga_kwargs: GA configuration\n",
    "        n_jobs: Number of parallel jobs (-1 for all cores)\n",
    "        \n",
    "    Returns:\n",
    "        (all_best, all_best_hist, all_avg_hist)\n",
    "    \"\"\"\n",
    "   \n",
    "    # Generate bootstrap samples\n",
    "    bootstrap_samples = parallel_year_bootstrap(dataset, n_samples=n_bootstraps, n_jobs=n_jobs)\n",
    "    \n",
    "    # Run GA in parallel\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(parallel_ga_wrapper)(sample) \n",
    "        for sample in tqdm(bootstrap_samples, desc=\"GA Optimization\"))\n",
    "    \n",
    "    # Unpack results\n",
    "    all_best = [r[0] for r in results]\n",
    "    all_best_hist = [r[1] for r in results]\n",
    "    all_avg_hist = [r[2] for r in results]\n",
    "    \n",
    "    return all_best, all_best_hist, all_avg_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91417468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
